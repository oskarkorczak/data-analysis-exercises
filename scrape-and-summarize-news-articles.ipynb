{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScrapeAndSummarizeNewsArticles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX3hyD-Mhrql"
      },
      "source": [
        "# This notebook aims to scrape and summarize newspaper articles with NLP support."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We5SNB39iEae"
      },
      "source": [
        "# Download and install libraries (do it only once and then comment out)\n",
        "# !pip install nltk\n",
        "# !pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMIgG6ggiko-"
      },
      "source": [
        "# Import libraries\n",
        "import nltk\n",
        "from newspaper import Article"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UHGgAQYjHcm"
      },
      "source": [
        "# Fetch the article\n",
        "url = 'https://www.washingtonpost.com/technology/2019/07/17/you-downloaded-faceapp-heres-what-youve-just-done-your-privacy/'\n",
        "article = Article(url)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf1QAGqioCRj"
      },
      "source": [
        "# Article NLP processing\n",
        "\n",
        "article.download()\n",
        "article.parse()\n",
        "#Punkt Sentence Tokenizer. This tokenizer divides a text into a list of sentences, \n",
        "# by using an unsupervised algorithm to build a model for abbreviation words, collocations,\n",
        "# and words that start sentences. It must be trained on a large collection of plaintext\n",
        "# in the target language before it can be used.\n",
        "nltk.download('punkt') \n",
        "article.nlp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OztsgB0DoyO8"
      },
      "source": [
        "# Obtain authors\n",
        "article.authors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JExMAnblqXkq"
      },
      "source": [
        "# Obtain publishing date\n",
        "article.publish_date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4OGUuBFrWrM"
      },
      "source": [
        "# Obtain top image\n",
        "article.top_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDTIGYanrkfx"
      },
      "source": [
        "# Obtain article text\n",
        "print(article.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liId61W5r9kG"
      },
      "source": [
        "# Obtain article summary\n",
        "print(article.summary)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}